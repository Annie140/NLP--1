{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beea8271",
   "metadata": {},
   "source": [
    "### 1.Correct the Search Query\n",
    "### Explanation: Here is a basic implementation using Python, focusing on spell correction using edit distance and a predefined corpus of words. This code uses zlib for compression and pickle for serialization, suitable for building an offline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d9de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries: 2\n",
      "Enter query: giong to chnia\n",
      "Enter query: firzt preisdent of india\n",
      "going to china\n",
      "first president of india\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import zlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Build corpus from a sample dictionary (you can enhance it with more words)\n",
    "words = \"\"\"going to china who was the first president of india winner of the match food in america\"\"\"\n",
    "\n",
    "\n",
    "def words_list(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Create a frequency dictionary from the words\n",
    "WORDS = Counter(words_list(words))\n",
    "\n",
    "# Compress and save the dictionary to a file\n",
    "with open('compressed_dict.pkl', 'wb') as f:\n",
    "    compressed = zlib.compress(pickle.dumps(WORDS))\n",
    "    f.write(compressed)\n",
    "\n",
    "# Load the dictionary from the file\n",
    "def load_dictionary():\n",
    "    with open('compressed_dict.pkl', 'rb') as f:\n",
    "        return pickle.loads(zlib.decompress(f.read()))\n",
    "\n",
    "# Generate all words with an edit distance of 1\n",
    "def edit_distance_one(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# Filter words that are in the dictionary\n",
    "def known(words, dictionary):\n",
    "    return set(w for w in words if w in dictionary)\n",
    "\n",
    "# Generate candidates for the correction\n",
    "def candidates(word, dictionary):\n",
    "    return (\n",
    "        known([word], dictionary) or\n",
    "        known(edit_distance_one(word), dictionary) or\n",
    "        [word]\n",
    "    )\n",
    "\n",
    "# Find the best correction for a word\n",
    "def correct_word(word, dictionary):\n",
    "    return max(candidates(word, dictionary), key=dictionary.get)\n",
    "\n",
    "# Correct all words in a query\n",
    "def correct_query(query, dictionary):\n",
    "    return ' '.join(correct_word(word, dictionary) for word in query.split())\n",
    "\n",
    "# Main correction function\n",
    "if __name__ == \"__main__\":\n",
    "    dictionary = load_dictionary()\n",
    "    n = int(input(\"Enter the number of queries: \"))\n",
    "    queries = [input(\"Enter query: \").strip() for _ in range(n)]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(correct_query(query, dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989d7f2",
   "metadata": {},
   "source": [
    "### 2.Deterministic Url and HashTag Segmentation\n",
    "### Explanation: This approach aims to find the most likely and meaningful segmentation of the input strings based on the provided dictionary of words and the constraint of selecting the longest valid tokens from the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46869533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RISHITHAA\n",
      "Enter the number of test cases: 2\n",
      "Enter input string: thisisatest\n",
      "Segmentation for Input: this is a test\n",
      "Enter input string: www.examplefor.com\n",
      "Segmentation for Input: example for\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load words from words.txt into a set\n",
    "with open(\"words.txt\", \"r\") as file:\n",
    "    dictionary = set(word.strip().lower() for word in file.readlines())\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Check if the string is a number.\"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize(input_string, dictionary):\n",
    "    \"\"\"\n",
    "    Tokenize the input string using the longest match first approach.\n",
    "\n",
    "    Args:\n",
    "        input_string: The string to be tokenized.\n",
    "        dictionary: A set of valid words.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens from the input string.\n",
    "    \"\"\"\n",
    "    length = len(input_string)\n",
    "    if length == 0:\n",
    "        return []\n",
    "\n",
    "    # dp[i] stores the tokens for the substring starting from index i\n",
    "    dp = [None] * (length + 1)\n",
    "    dp[0] = []  # Base case: empty string has no tokens\n",
    "\n",
    "    for i in range(1, length + 1):\n",
    "        # Consider all possible ending positions for the current substring\n",
    "        for j in range(i):\n",
    "            left_part = input_string[j:i]\n",
    "\n",
    "            # Check if left part is a valid word or number\n",
    "            if (left_part in dictionary or is_number(left_part)) and (dp[j] is not None):\n",
    "                # If left part is valid and remaining part has a valid tokenization\n",
    "                right_part_tokens = dp[j] + [left_part]\n",
    "\n",
    "                # Choose the longest valid tokenization\n",
    "                if dp[i] is None or len(right_part_tokens) > len(dp[i]):\n",
    "                    dp[i] = right_part_tokens\n",
    "\n",
    "    # Return the tokenization for the entire string if it exists\n",
    "    return dp[length] if dp[length] is not None else [input_string]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Read input strings, tokenize them, and print the results.\"\"\"\n",
    "    num_test_cases = int(input(\"Enter the number of test cases: \"))\n",
    "    for _ in range(num_test_cases):\n",
    "        input_string = input(\"Enter input string: \").strip().lower()\n",
    "\n",
    "        # Remove www and extensions for domain names, # for hashtags\n",
    "        if input_string.startswith(\"www.\"):\n",
    "            input_string = input_string[4:].rsplit(\".\", 1)[0]\n",
    "        elif input_string.startswith(\"#\"):\n",
    "            input_string = input_string[1:]\n",
    "\n",
    "        tokens = tokenize(input_string, dictionary)\n",
    "        print(f\"Segmentation for Input: {' '.join(tokens)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be896",
   "metadata": {},
   "source": [
    "### 3.Disambiguation: Mouse vs Mouse\n",
    "### Explanation: This code provides a basic framework for classifying the usage of the word \"mouse\" in a sentence. You can further improve the accuracy by:\n",
    "### Expanding the Training Data: Use a larger and more diverse dataset of sentences.\n",
    "### Experimenting with Different Classifiers: Try other machine learning models like Support Vector Machines (SVM) or Random Forests.\n",
    "### Using Word Embeddings: Consider using word embeddings like Word2Vec or GloVe to capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7298ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of test cases: 3\n",
      "Enter a sentence: The mouse has a long tail.\n",
      "animal\n",
      "Enter a sentence: I connected the mouse to my laptop.\n",
      "computer-mouse\n",
      "Enter a sentence: The house was infested with mice.\n",
      "animal\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training data (sample corpus)\n",
    "training_sentences = [\n",
    "    \"The complete mouse reference genome was sequenced in 2002.\",\n",
    "    \"Tail length varies according to the environmental temperature of the mouse during postnatal development.\",\n",
    "    \"A mouse is an input device.\",\n",
    "    \"Many mice have a pink tail.\",\n",
    "    \"The mouse pointer on the screen helps in navigation.\",\n",
    "    \"A rodent like a mouse has sharp teeth.\",\n",
    "    \"The mouse was connected to the computer using a USB port.\",\n",
    "    \"The house was infested with mice.\",\n",
    "    \"Computer users often prefer a wireless mouse.\"\n",
    "]\n",
    "\n",
    "# Labels corresponding to the training sentences\n",
    "labels = [\n",
    "    \"animal\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\"\n",
    "]\n",
    "\n",
    "# Vectorize the training sentences\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(training_sentences)\n",
    "\n",
    "# Create and train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, labels)\n",
    "\n",
    "# Function to predict the type of \"mouse\"\n",
    "def predict_mouse_type(sentence):\n",
    "    \"\"\"\n",
    "    Predicts whether the 'mouse' in the sentence refers to an animal or a computer mouse.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence.\n",
    "\n",
    "    Returns:\n",
    "        \"animal\" or \"computer-mouse\"\n",
    "    \"\"\"\n",
    "    vectorized_sentence = vectorizer.transform([sentence])\n",
    "    prediction = classifier.predict(vectorized_sentence)[0]\n",
    "    return prediction\n",
    "\n",
    "# Get number of test cases\n",
    "num_test_cases = int(input(\"Enter the number of test cases: \"))\n",
    "\n",
    "# Process each test case\n",
    "for _ in range(num_test_cases):\n",
    "    sentence = input(\"Enter a sentence: \")\n",
    "    prediction = predict_mouse_type(sentence)\n",
    "    print(prediction)\n",
    "\n",
    "# Optionally, save the trained model for later use\n",
    "with open('mouse_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump((vectorizer, classifier), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c59c3",
   "metadata": {},
   "source": [
    "### 4.Language Detection\n",
    "### Explanation: This function loads the pre-trained model from a serialized file.\n",
    "### It takes a text snippet as input, normalizes it to ASCII, and converts it into a TF- IDF vector using the loaded vectorizer.\n",
    "### The function then uses the trained classifier to predict the language of the snippet based on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7135221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a text snippet to detect the language: El rápido zorro marrón salta sobre el perro perezoso.\n",
      "Detected language: Spanish\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def normalize_to_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters and normalize text.\"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "\n",
    "# Step 1: Training Data\n",
    "training_texts = {\n",
    "    \"English\": [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Rip Van Winkle is a story set in the years before the American Revolutionary War.\",\n",
    "        \"Hello, how are you today?\",\n",
    "        \"It is a wonderful day to learn something new.\"\n",
    "    ],\n",
    "    \"French\": [\n",
    "        \"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
    "        \"La revolution francaise a marque une periode importante de l'histoire.\",\n",
    "        \"Bonjour, comment ça va?\",\n",
    "        \"Il est temps de découvrir de nouvelles choses.\"\n",
    "    ],\n",
    "    \"German\": [\n",
    "        \"Der schnelle braune Fuchs springt über den faulen Hund.\",\n",
    "        \"Die deutsche Wiedervereinigung war ein historisches Ereignis.\",\n",
    "        \"Hallo, wie geht es dir?\",\n",
    "        \"Es ist ein wunderbarer Tag, um etwas Neues zu lernen.\"\n",
    "    ],\n",
    "    \"Spanish\": [\n",
    "        \"El rapido zorro marron salta sobre el perro perezoso.\",\n",
    "        \"La Revolucion Espanola fue un momento clave en la historia.\",\n",
    "        \"Hola, ¿cómo estás?\",\n",
    "        \"Es un gran día para aprender algo nuevo.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# Normalize training data to ASCII\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for language, samples in training_texts.items():\n",
    "    labels.extend([language] * len(samples))\n",
    "    texts.extend([normalize_to_ascii(sample) for sample in samples])\n",
    "\n",
    "\n",
    "# Step 2: Preprocessing and Feature Extraction\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 4), analyzer=\"char\")\n",
    "X_train = vectorizer.fit_transform(texts)\n",
    "\n",
    "\n",
    "# Step 3: Train the Model\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, labels)\n",
    "\n",
    "\n",
    "# Step 4: Serialize the Model\n",
    "with open(\"language_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump((vectorizer, classifier), model_file)\n",
    "\n",
    "\n",
    "# Step 5: Language Detection Function\n",
    "def detect_language(snippet):\n",
    "    with open(\"language_model.pkl\", \"rb\") as model_file:\n",
    "        vectorizer, classifier = pickle.load(model_file)\n",
    "\n",
    "    # Normalize snippet to ASCII\n",
    "    snippet = normalize_to_ascii(snippet)\n",
    "    X_test = vectorizer.transform([snippet])\n",
    "    prediction = classifier.predict(X_test)\n",
    "    return prediction[0]\n",
    "\n",
    "\n",
    "# Input Processing (Single-line input)\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input\n",
    "    snippet = input(\"Enter a text snippet to detect the language: \")\n",
    "\n",
    "    # Predict and Output\n",
    "    detected_language = detect_language(snippet.strip())\n",
    "    print(f\"Detected language: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac937fd4",
   "metadata": {},
   "source": [
    "### 5.The Missing Apostrophes\n",
    "### Explanation Apostrophe Handling: The code defines a function restore_apostrophes that iterates through each word in the input text. It uses a combination of explicit checks for common contractions (e.g., \"don't,\" \"can't,\" \"I've\") and a regular expression to handle possessive nouns (e.g., \"cat's,\" \"dog's\") to restore apostrophes where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6bc8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At a new's conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on hi's previou's trip into space in 2010 \"I even asked our psychological support folk's to send me a calendar with photograph's of nature, of rivers, of woods, of lakes.\" Kelly wa's asked if hed mis's hi's twin brother Mark, who also wa's an astronaut. \"Were used to thi's kind of thing,\" he said. \"Ive gone longer without seeing him and it wa's great.\" The mission won't be the longest time that a human ha's spent in space - four Russian's spent a year or more aboard the Soviet-built Mir space station in the 1990s. SCI Astronaut Twin's Scott Kelly (left) wa's asked Thursday if hed mis's hi's twin brother, Mark, who also wa's an astronaut. we're used to thi's kind of thing, he said. I've gone longer without seeing him and it wa's great. (NASA/Associated Press) \"The last time we had such a long duration flight wa's almost 20 year's and of course al{-truncated-}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to handle apostrophes for contractions and possessives\n",
    "def restore_apostrophes(text):\n",
    "    restored_text = []\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        lower_word = word.lower()\n",
    "\n",
    "        # Handle contractions\n",
    "        if lower_word == \"dont\":\n",
    "            restored_text.append(\"don't\")\n",
    "        elif lower_word == \"wont\":\n",
    "            restored_text.append(\"won't\")\n",
    "        elif lower_word == \"cant\":\n",
    "            restored_text.append(\"can't\")\n",
    "        elif lower_word == \"isnt\":\n",
    "            restored_text.append(\"isn't\")\n",
    "        elif lower_word == \"arent\":\n",
    "            restored_text.append(\"aren't\")\n",
    "        elif lower_word == \"wasnt\":\n",
    "            restored_text.append(\"wasn't\")\n",
    "        elif lower_word == \"werent\":\n",
    "            restored_text.append(\"weren't\")\n",
    "        elif lower_word == \"hasnt\":\n",
    "            restored_text.append(\"hasn't\")\n",
    "        elif lower_word == \"havent\":\n",
    "            restored_text.append(\"haven't\")\n",
    "        elif lower_word == \"hadnt\":\n",
    "            restored_text.append(\"hadn't\")\n",
    "        elif lower_word == \"didnt\":\n",
    "            restored_text.append(\"didn't\")\n",
    "        elif lower_word == \"ive\":\n",
    "            restored_text.append(\"I've\")\n",
    "        elif lower_word == \"were\":\n",
    "            restored_text.append(\"we're\")\n",
    "        elif lower_word == \"i\":\n",
    "            restored_text.append(\"I\")\n",
    "        elif lower_word == \"id\":\n",
    "            restored_text.append(\"I'd\")\n",
    "        elif lower_word == \"youve\":\n",
    "            restored_text.append(\"you've\")\n",
    "        elif lower_word == \"hes\":\n",
    "            restored_text.append(\"he's\")\n",
    "        elif lower_word == \"shes\":\n",
    "            restored_text.append(\"she's\")\n",
    "        elif lower_word == \"its\":\n",
    "            restored_text.append(\"it's\")\n",
    "        elif lower_word == \"were\":\n",
    "            restored_text.append(\"we're\")\n",
    "\n",
    "        # Handle possessives (only add 's when it makes sense)\n",
    "        elif re.match(r'\\w+s$', word) and lower_word not in [\"its\", \"hers\", \"ours\", \"yours\", \"theirs\"]:\n",
    "            restored_text.append(re.sub(r\"s$\", \"'s\", word))\n",
    "\n",
    "        # For normal words that don't need apostrophes, keep them as is\n",
    "        else:\n",
    "            restored_text.append(word)\n",
    "\n",
    "    return \" \".join(restored_text)\n",
    "\n",
    "\n",
    "# Input\n",
    "input_text = \"\"\"At a news conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on his previous trip into space in 2010 \"I even asked our psychological support folks to send me a calendar with photographs of nature, of rivers, of woods, of lakes.\"\n",
    "Kelly was asked if hed miss his twin brother Mark, who also was an astronaut.\n",
    "\n",
    "\"Were used to this kind of thing,\" he said. \"Ive gone longer without seeing him and it was great.\"\n",
    "The mission wont be the longest time that a human has spent in space - four Russians spent a year or more aboard the Soviet-built Mir space station in the 1990s.\n",
    "SCI Astronaut Twins\n",
    "Scott Kelly (left) was asked Thursday if hed miss his twin brother, Mark, who also was an astronaut. Were used to this kind of thing, he said. Ive gone longer without seeing him and it was great. (NASA/Associated Press)\n",
    "\"The last time we had such a long duration flight was almost 20 years and of course al{-truncated-}\"\"\"\n",
    "\n",
    "# Restore apostrophes\n",
    "output_text = restore_apostrophes(input_text)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a00705",
   "metadata": {},
   "source": [
    "### 6.Segment the Twitter Hashtags\n",
    "### Explanation: Tokenization with Dynamic Programming: The segment_hashtag function uses dynamic programming to break down the hashtag into a sequence of words. It iterates through the hashtag, checking for valid word combinations from a given dictionary and selecting the longest possible valid sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0dccac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of hashtags: 3\n",
      "Enter hashtag 1: wearethepeople\n",
      "Enter hashtag 2: mentionyourfaves\n",
      "Enter hashtag 3: playingwalkingdead\n",
      "we are the people\n",
      "mention your faves\n",
      "playing walking dead\n"
     ]
    }
   ],
   "source": [
    "# Define a function that segments a single hashtag into words\n",
    "def segment_hashtag(hashtag, word_dict):\n",
    "    n = len(hashtag)\n",
    "    dp = [None] * (n + 1)\n",
    "\n",
    "    dp[0] = []  # Base case: empty string can be segmented as an empty list\n",
    "\n",
    "    # Iterate over the hashtag string\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(max(0, i - 20), i):  # Limit the length of words checked\n",
    "            word = hashtag[j:i]\n",
    "            if word in word_dict and dp[j] is not None:\n",
    "                dp[i] = dp[j] + [word]\n",
    "                break\n",
    "\n",
    "    return \" \".join(dp[n]) if dp[n] is not None else hashtag\n",
    "\n",
    "\n",
    "# Main function to process input and output results\n",
    "def process_hashtags(num_hashtags, hashtags, word_dict):\n",
    "    result = []\n",
    "    for hashtag in hashtags:\n",
    "        segmented = segment_hashtag(hashtag, word_dict)\n",
    "        result.append(segmented)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Sample dictionary of common words (expand this as needed)\n",
    "word_dict = {\n",
    "    \"we\", \"are\", \"the\", \"people\", \"mention\", \"your\", \"faves\",\n",
    "    \"now\", \"playing\", \"walking\", \"dead\", \"follow\", \"me\"\n",
    "}\n",
    "\n",
    "\n",
    "# Sample input\n",
    "num_hashtags = int(input(\"Enter the number of hashtags: \"))\n",
    "hashtags = [input(f\"Enter hashtag {i + 1}: \").strip() for i in range(num_hashtags)]\n",
    "\n",
    "\n",
    "# Process the hashtags and print the result\n",
    "segmented_hashtags = process_hashtags(num_hashtags, hashtags, word_dict)\n",
    "for segmented in segmented_hashtags:\n",
    "    print(segmented)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908910f",
   "metadata": {},
   "source": [
    "### 7.Expand the Acronyms\n",
    "### Explanation: Acronym Extraction: The code extracts acronyms and their potential expansions from a given set of text snippets by identifying uppercase words within parentheses and searching for preceding phrases. It also attempts to extract acronyms not explicitly defined in parentheses by analyzing the surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc4de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of snippets: 1\n",
      "Enter snippet 1: The system of Local Area Network (LAN) allows communication.\n",
      "Enter test acronym 1: LAN\n",
      "Processing snippet: The system of Local Area Network (LAN) allows communication.\n",
      "Found acronyms in parentheses: ['LAN']\n",
      "Captured explicit expansion for (LAN): system of Local Area Network\n",
      "Processing test acronym: LAN, found expansion: Not Found\n",
      "\n",
      "Results:\n",
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_acronyms_and_expansions(snippets):\n",
    "    \"\"\"\n",
    "    Extract acronyms and their expansions from the provided snippets.\n",
    "    \"\"\"\n",
    "    acronym_dict = {}\n",
    "    \n",
    "    for snippet in snippets:\n",
    "        print(f\"Processing snippet: {snippet}\")\n",
    "        \n",
    "        # 1. Find all potential acronyms (uppercase words typically enclosed in parentheses)\n",
    "        matches = re.findall(r'\\(([^)]+)\\)', snippet)  # Capture everything inside parentheses\n",
    "        print(f\"Found acronyms in parentheses: {matches}\")\n",
    "        \n",
    "        for match in matches:\n",
    "            # Split the match by spaces to capture the acronym and the expansion\n",
    "            acronym_expansion = match.split(' ', 1)\n",
    "            if len(acronym_expansion) == 2:  # If we have both acronym and expansion\n",
    "                acronym = acronym_expansion[0].strip()\n",
    "                expansion = acronym_expansion[1].strip()\n",
    "                acronym_dict[acronym] = expansion\n",
    "                print(f\"Captured acronym-expansion pair: {acronym} -> {expansion}\")\n",
    "        \n",
    "        # 2. Handle acronyms not in parentheses but defined explicitly (case-sensitive)\n",
    "        words = snippet.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.isupper() and len(word) > 1:  # Likely an acronym\n",
    "                if word not in acronym_dict:\n",
    "                    # Try to extract its expansion from the surrounding context\n",
    "                    preceding_context = \" \".join(words[max(0, i-5):i])\n",
    "                    if preceding_context:\n",
    "                        acronym_dict[word] = preceding_context.strip()\n",
    "                        print(f\"Captured explicit expansion for {word}: {preceding_context}\")\n",
    "    \n",
    "    return acronym_dict\n",
    "\n",
    "\n",
    "def process_tests(acronym_dict, tests):\n",
    "    \"\"\"\n",
    "    Process test acronyms and return their expansions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test in tests:\n",
    "        # Normalize the test acronym (case insensitive)\n",
    "        expansion = acronym_dict.get(test.upper(), \"Not Found\")\n",
    "        print(f\"Processing test acronym: {test}, found expansion: {expansion}\")\n",
    "        results.append(expansion)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read input\n",
    "    n = int(input(\"Enter number of snippets: \").strip())\n",
    "    \n",
    "    snippets = [input(f\"Enter snippet {i + 1}: \").strip() for i in range(n)]\n",
    "    tests = [input(f\"Enter test acronym {i + 1}: \").strip() for i in range(n)]\n",
    "    \n",
    "    # Extract acronyms and expansions\n",
    "    acronym_dict = extract_acronyms_and_expansions(snippets)\n",
    "    \n",
    "    # Process test queries\n",
    "    results = process_tests(acronym_dict, tests)\n",
    "    \n",
    "    # Output results\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58ee9b",
   "metadata": {},
   "source": [
    "### 8.Correct the Search Query\n",
    "#### Explanation: Here is a basic implementation using Python, focusing on spell correction using edit distance and a predefined corpus of words. This code uses zlib for compression and pickle for serialization, suitable for building an offline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee4d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries: 2\n",
      "Enter query 1: goin to china\n",
      "Enter query 2: winer of the match\n",
      "going to china\n",
      "winner of the match\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import zlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Build corpus from a sample dictionary (you can enhance it with more words)\n",
    "words = \"\"\"going to china who was the first president of india winner of the match food in america\"\"\"\n",
    "\n",
    "# Function to return a list of words from the text\n",
    "def words_list(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Count words in the corpus\n",
    "WORDS = Counter(words_list(words))\n",
    "\n",
    "# Compression for large wordlist\n",
    "with open('compressed_dict.pkl', 'wb') as f:\n",
    "    compressed = zlib.compress(pickle.dumps(WORDS))\n",
    "    f.write(compressed)\n",
    "\n",
    "# Load dictionary in memory\n",
    "def load_dictionary():\n",
    "    with open('compressed_dict.pkl', 'rb') as f:\n",
    "        return pickle.loads(zlib.decompress(f.read()))\n",
    "\n",
    "# Generate a set of possible words with a single edit distance\n",
    "def edit_distance_one(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# Known words in the dictionary\n",
    "def known(words, dictionary):\n",
    "    return set(w for w in words if w in dictionary)\n",
    "\n",
    "# Candidates for correction based on edit distance\n",
    "def candidates(word, dictionary):\n",
    "    return (known([word], dictionary) or known(edit_distance_one(word), dictionary) or [word])\n",
    "\n",
    "# Correct a single word\n",
    "def correct_word(word, dictionary):\n",
    "    return max(candidates(word, dictionary), key=dictionary.get)\n",
    "\n",
    "# Correct a whole query\n",
    "def correct_query(query, dictionary):\n",
    "    return ' '.join(correct_word(word, dictionary) for word in query.split())\n",
    "\n",
    "# Main correction function\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dictionary\n",
    "    dictionary = load_dictionary()\n",
    "    \n",
    "    # Input number of queries\n",
    "    n = int(input(\"Enter the number of queries: \").strip())\n",
    "    \n",
    "    # Input queries\n",
    "    queries = [input(f\"Enter query {i+1}: \").strip() for i in range(n)]\n",
    "    \n",
    "    # Process each query and correct it\n",
    "    for query in queries:\n",
    "        print(correct_query(query, dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c1c17",
   "metadata": {},
   "source": [
    "### 9.A Text-Processing Warmup\n",
    "### Explanation: Article and Date Counting: The code defines a function count_articles_and_dates that takes a text fragment as input. It first normalizes the text to lowercase for case-insensitive article counting. Then, it uses regular expressions to count occurrences of the definite and indefinite articles (\"a,\" \"an,\" \"the\") and identify valid dates in various formats (e.g., \"DD Month YYYY,\" \"Month DD, YYYY,\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748455ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment: I went to the park on 5th January 2023.\n",
      "a_count: 0, an_count: 0, the_count: 1\n",
      "Found dates: [('January', '')]\n",
      "Fragment: She arrived on 12/02/2021 and met John on January 15, 2021.\n",
      "a_count: 0, an_count: 0, the_count: 0\n",
      "Found dates: [('', ''), ('', 'January')]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def count_articles_and_dates(fragment):\n",
    "    \"\"\"\n",
    "    Count occurrences of 'a', 'an', 'the', and valid dates in a given text fragment.\n",
    "    \"\"\"\n",
    "    # Normalize text for article counting\n",
    "    lower_fragment = fragment.lower()\n",
    "\n",
    "    # Count articles (simplified to handle punctuation better)\n",
    "    a_count = len(re.findall(r'\\ba\\b', lower_fragment))\n",
    "    an_count = len(re.findall(r'\\ban\\b', lower_fragment))\n",
    "    the_count = len(re.findall(r'\\bthe\\b', lower_fragment))\n",
    "\n",
    "    # Debugging: Print counts of articles\n",
    "    print(f\"Fragment: {fragment}\")\n",
    "    print(f\"a_count: {a_count}, an_count: {an_count}, the_count: {the_count}\")\n",
    "\n",
    "    # Identify valid dates\n",
    "    date_patterns = [\n",
    "        # Day Month Year (e.g., 5th January 2023)\n",
    "        r'\\b\\d{1,2}(?:st|nd|rd|th)?(?:\\s+of)?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4}\\b',  \n",
    "        \n",
    "        # Month Day Year (e.g., January 15, 2021)\n",
    "        r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd)?(?:,?)?\\s+\\d{2,4}\\b',  \n",
    "        \n",
    "        # Day/Month/Year (e.g., 12/02/2021)\n",
    "        r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',  # Day/Month/Year\n",
    "        \n",
    "        # ISO format: Year-Month-Day (e.g., 2021-12-15)\n",
    "        r'\\b\\d{4}-\\d{2}-\\d{2}\\b'  # ISO format: Year-Month-Day\n",
    "    ]\n",
    "\n",
    "    # Combine all date patterns\n",
    "    date_regex = '|'.join(date_patterns)\n",
    "    dates = re.findall(date_regex, fragment, re.IGNORECASE)\n",
    "    \n",
    "    # Debugging: Print found dates\n",
    "    print(f\"Found dates: {dates}\")\n",
    "\n",
    "    date_count = len(dates)\n",
    "\n",
    "    return a_count, an_count, the_count, date_count\n",
    "\n",
    "def main():\n",
    "    # Directly simulate input for testing (use your own test cases here)\n",
    "    data = \"\"\"2\n",
    "I went to the park on 5th January 2023.\n",
    "She arrived on 12/02/2021 and met John on January 15, 2021.\"\"\"\n",
    "    \n",
    "    data = data.strip().split(\"\\n\")\n",
    "    \n",
    "    # Proceed if data is not empty\n",
    "    if not data:\n",
    "        print(\"Error: No input data provided\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        t = int(data[0].strip())  # Number of test cases\n",
    "    except ValueError:\n",
    "        print(\"Error: Invalid number of test cases\")\n",
    "        return\n",
    "\n",
    "    fragments = data[1:]  # Remaining lines contain the fragments\n",
    "\n",
    "    if len(fragments) != t:\n",
    "        print(f\"Error: Expected {t} fragments, but got {len(fragments)}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    for i in range(t):\n",
    "        fragment = fragments[i].strip()  # Count articles and dates\n",
    "        a_count, an_count, the_count, date_count = count_articles_and_dates(fragment)\n",
    "        results.append(f\"{a_count}\\n{an_count}\\n{the_count}\\n{date_count}\")\n",
    "\n",
    "    # Output results\n",
    "    print(\"\\n\".join(results))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c729bd6",
   "metadata": {},
   "source": [
    "### Who is it?\n",
    "### Explanation: Pronoun Identification and Entity Matching: The code first finds all pronouns (words enclosed in double backslashes) and their positions in the text. It then cleans the text by removing the backslashes. Next, it iterates through each pronounand searches for the closest matching entity (from a provided list) that appears before the pronoun in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6feed1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of text snippets:\n",
      "2\n",
      "Enter 2 lines of text:\n",
      "John and Mary went to the park.\n",
      "She gave him a gift.\n",
      "Enter entities (separated by ';'):\n",
      "John; Mary\n",
      "John\n",
      "John\n",
      "Mary\n",
      "Mary\n",
      "Mary\n",
      "Mary\n",
      "Mary\n",
      "Mary\n",
      "Mary\n",
      "Mary\n",
      "Mary\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def resolve_pronouns(text, entities):\n",
    "    \"\"\"\n",
    "    Resolves pronouns in the text based on provided entities.\n",
    "    \"\"\"\n",
    "    # Extract all pronouns and their positions\n",
    "    pronoun_pattern = r'\\b(\\w+)\\b'  # Updated pattern to match any word\n",
    "    pronouns = [(match.group(1), match.start()) for match in re.finditer(pronoun_pattern, text)]\n",
    "\n",
    "    # Clean the text by removing ** markers (if any)\n",
    "    clean_text = re.sub(r'\\*\\*(\\w+)\\*\\*', r'\\1', text)\n",
    "\n",
    "    # Initialize a list to store the resolved entities\n",
    "    resolved = []\n",
    "\n",
    "    # For each pronoun, find the corresponding entity\n",
    "    for pronoun, pos in pronouns:\n",
    "        closest_entity = None\n",
    "        closest_distance = float('inf')\n",
    "\n",
    "        # Iterate through all entities to find the best match for the pronoun\n",
    "        for entity in entities:\n",
    "            entity_pos = clean_text.rfind(entity, 0, pos)  # Find the last occurrence of the entity before the pronoun\n",
    "            if entity_pos != -1:\n",
    "                distance = pos - (entity_pos + len(entity))\n",
    "                if distance < closest_distance:\n",
    "                    closest_distance = distance\n",
    "                    closest_entity = entity\n",
    "\n",
    "        # Append the resolved entity to the list\n",
    "        if closest_entity:\n",
    "            resolved.append(closest_entity)\n",
    "\n",
    "    return resolved\n",
    "\n",
    "def main():\n",
    "    # Read input interactively\n",
    "    print(\"Enter number of text snippets:\")\n",
    "    n = int(input().strip())  # Read number of snippets\n",
    "\n",
    "    print(f\"Enter {n} lines of text:\")\n",
    "    text_snippet = \" \".join(input().strip() for _ in range(n))  # Read text snippet lines\n",
    "\n",
    "    print(\"Enter entities (separated by ';'):\")\n",
    "    entities = [e.strip() for e in input().strip().split(';')]  # Read list of entities\n",
    "\n",
    "    # Resolve pronouns\n",
    "    result = resolve_pronouns(text_snippet, entities)\n",
    "\n",
    "    # Output the resolved entities\n",
    "    for entity in result:\n",
    "        print(entity)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac5a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640214c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
